{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tweaking up semantic retrieval\n",
    "\n",
    "There are various objectives we could try optimizing for when it comes to semantic retrieval. We could try to optimize the **speed** of the retrieval, the **quality** of it, or the **memory usage**. We'll review some of the techniques in all three areas."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5527397e89ce8f7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the configuration and pipeline\n",
    "\n",
    "Again, let's start with loading the configuration, and then set up our retriever. We don't want a full RAG pipeline, as we are solely interested in the semantic search part. Improving a single component at a time should be easier to understand and debug. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c97156bfd207c831"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5400644c6fa94d96",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=\"local:BAAI/bge-large-en\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c571d2c60524ef3d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "import os\n",
    "\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    collection_name=\"hacker-news\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "480e88c42e30d3cd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    service_context=service_context,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb26026d9b2d8e0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import MetadataFilters, MetadataFilter\n",
    "from llama_index.indices.vector_store import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    "    filters=MetadataFilters(\n",
    "        filters=[\n",
    "            MetadataFilter(key=\"type\", value=\"story\"),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a44471047edccc2a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "nodes = retriever.retrieve(\"What is the best way to learn programming?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37fefadc69369516",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quality optimization\n",
    "\n",
    "We have implemented a basic RAG already, and we might be happy with the quality. There are a lot of aspects when it comes to measuring the quality of a semantic retrieval system, and we will not go into details here. It is usually related to the quality of the embedding model we use, and it is a topic for another day.\n",
    "\n",
    "However, all the vector databases approximate the nearest neighbor search, and this approximation comes with a cost. The cost is that the results are not always ideal. HNSW, an algorithm used in Qdrant, has some parameters to control how the internal structures are built, and these parameters can be tweaked to improve the quality of the results. This is very specific to the vector database used, thus it's configured through the Qdrant API."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c8084e400f1ff37"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "client.get_collection(collection_name=\"hacker-news\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18105b7cfb654cb6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As for now, the most interesting part is the `hnsw_config` field. The algorithm itself is controlled by two parameters. The number of edges per node is called the `m` parameter. The larger the value, the higher the precision of the search, but the more space required. The `ef_construct` parameter is the number of neighbors to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time. \n",
    "\n",
    "Playing with both parameters **improves just the approximation of the exact nearest neighbors**, and a proper embedding model is still way more important. However, [this quality aspect might also be controlled, even in an automated way](https://qdrant.tech/documentation/tutorials/retrieval-quality/). For the time being, we'll simply increase both values, but won't measure the impact on the overall quality of search results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a7eb35651edebd8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "client.update_collection(\n",
    "    collection_name=\"hacker-news\",\n",
    "    hnsw_config=models.HnswConfigDiff(\n",
    "        m=32,\n",
    "        ef_construct=200,\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3668e7f04e38d00f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    collection = client.get_collection(\"hacker-news\")\n",
    "    if collection.status == models.CollectionStatus.GREEN:\n",
    "        break\n",
    "    time.sleep(1.0)\n",
    "        \n",
    "collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c4d08884025a701",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "nodes = retriever.retrieve(\"What is the best way to learn programming?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a2ede02bc5281ec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Memory optimization\n",
    "\n",
    "Each point in a Qdrant collection consists of up to three elements: id, vector(s), and optional payload represented by a JSON object. Vectors are indexed in an HNSW graph, and search operations may involve semantic similarity and some payload-based criteria (it's best to add payload indexes on the fields we want to use for the filtering). Ideally, all the elements should be kept in RAM so access is fast.\n",
    "\n",
    "Unfortunately, semantic search is a heavy operation in terms of memory requirements. However, some projects are implemented on a budget and can't afford machines with hundreds of gigabytes of RAM. Qdrant allows storing every single component on a disk to reduce memory usage, but that comes with a performance cost. Let's compare the efficiency of the operations with all the components in RAM and with some of them on disk."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd6b5c8fd6513bb5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%timeit -n 100 -r 5\n",
    "retriever.retrieve(\"What is the best way to learn programming?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d2159e4871b1dd6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "client.update_collection(\n",
    "    collection_name=\"hacker-news\",\n",
    "    hnsw_config=models.HnswConfigDiff(\n",
    "        on_disk=True,\n",
    "    ),\n",
    "    vectors_config={\n",
    "        \"\": models.VectorParamsDiff(\n",
    "            on_disk=True,\n",
    "        )\n",
    "    },\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1534ffc86ce1f7e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "while True:\n",
    "    collection = client.get_collection(\"hacker-news\")\n",
    "    if collection.status == models.CollectionStatus.GREEN:\n",
    "        break\n",
    "    time.sleep(1.0)\n",
    "        \n",
    "collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396074dc50565c7a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%timeit -n 100 -r 5\n",
    "retriever.retrieve(\"What is the best way to learn programming?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f500d12db05fdc6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Speed optimization\n",
    "\n",
    "There are various ways of optimizing semantic search in terms of speed. The most straightforward one is to reduce both `m` and `ef_construct` parameters, as we did in the previous section. However, this comes with a cost of the quality of the results.\n",
    "\n",
    "Qdrant also provides a number of quantization techniques, and two of them are primarily used to increase speed and reduce memory at the same time:\n",
    "\n",
    "1. **Scalar Quantization** - uses `int8` instead of `float32` to store each vector dimension\n",
    "2. **Binary Quantization** - `bool` values are used to store each vector dimension\n",
    "\n",
    "The first one reduces the memory usage by up to 4x, while the second one by up to 32x and both increase the speed of the search. However, the quality of the search results is reduced, and Binary Quantization is not suitable for all the use cases. It only works with some specific models, usually the ones with high dimensionality."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8be9fc24e922f165"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our case, we're going to set up the binary quantization either way. From the LlamaIndex perspective, the search operations are going to be fired identically."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1154f4172094cdf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "client.update_collection(\n",
    "    collection_name=\"hacker-news\",\n",
    "    quantization_config=models.BinaryQuantization(\n",
    "        binary=models.BinaryQuantizationConfig(\n",
    "            always_ram=True,\n",
    "        )\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e26ec00449bfb27",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "while True:\n",
    "    collection = client.get_collection(\"hacker-news\")\n",
    "    if collection.status == models.CollectionStatus.GREEN:\n",
    "        break\n",
    "    time.sleep(1.0)\n",
    "        \n",
    "collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a01cc56dff3ea252",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "nodes = retriever.retrieve(\"What is the best way to learn programming?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f4d07f587535769",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%timeit -n 100 -r 5\n",
    "retriever.retrieve(\"What is the best way to learn programming?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "408270c8449987c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "536dbd5e38b33efd",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
